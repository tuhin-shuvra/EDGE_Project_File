# -*- coding: utf-8 -*-
"""Spam_SMS_or_Email_Classifer_EDGE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fjo_MPIC0wdPjzHTUbVqO-XrQg4cSKvj

**Spam SMS Classifier Project For EDGE **
"""



import numpy as np
import pandas as pd

"""To read the files in Colab we have to mount the drive first"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/EDGE_Practice/Project_File/spam.csv', encoding='latin1')

#If you want to run the code in a local environment, skip the blocks above and follow the following:
#df = pd.read_csv("spam.csv")

df.sample(5)

df.shape

"""Our Workflow:


*   Data Cleaning
*   EDA
*   Text Preprocessing
*   Model Building
*   Evaluation
*   Improvement

**At first we will do data cleaning**
"""

df.info()

#drop last 3 columns as they contain very negligible data
df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)

df.sample(5)

#renaming the columns
 df.rename(columns={'v1':'target', 'v2':'text'}, inplace=True)
 df.sample(5)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['target'] = encoder.fit_transform(df['target'])

df.head()

#missing values
df.isnull().sum()

#checking for duplicate values
df.duplicated().sum()

#remove duplicates
df = df.drop_duplicates(keep='first')

df.duplicated().sum()

df.shape

"""The data cleaning part is done.
Now we will perform **EDA or Exploratory Data Analysis**
"""

df.head()

df['target'].value_counts()

#WE wanna represent it better using MatPlotlif

import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(), labels=['ham', 'spam'], autopct="%0.2f")
plt.show()

"""From here we can tell data is a litlle imbalance"""

#for the next step we need to install nltk
import nltk
nltk.download('punkt_tab')

#Number of char in a sms
df['num_characters'] = df['text'].apply(len)

df.head()

#number of words
df['num_words'] = df['text'].apply(lambda x: len(nltk.word_tokenize(x)))

#number of sentences

df['num_sentences'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))

df.head()

df[['num_characters', 'num_words', 'num_sentences']].describe()

#ham messages
df[df['target'] == 0] [['num_characters', 'num_words', 'num_sentences']].describe()

#spam
df[df['target'] == 1] [['num_characters', 'num_words', 'num_sentences']].describe()

#Now we wann see the classes in histogram

import seaborn as sns

sns.histplot(df[df['target'] == 0] ['num_characters'])
sns.histplot(df[df['target'] == 1] ['num_characters'], color='red')

#now lets see it regarding words

sns.histplot(df[df['target'] == 0] ['num_words'])
sns.histplot(df[df['target'] == 1] ['num_words'], color='red')

#co-rellation of number of sentences, words and char

sns.pairplot(df, hue='target')

#now let's represent with heatmap
# Select only numeric columns before calculating correlation
df_num = df.select_dtypes(include=['number'])

# Check if there are numeric columns
if df_num.shape[1] > 1:
    sns.heatmap(df_num.corr(), annot=True, cmap="coolwarm")
    plt.title("Correlation Heatmap")
    plt.show()
else:
    print("⚠️ Not enough numeric columns to create a heatmap.")

"""**Now we will do data or text preprocessing**"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
print(stopwords.words('english'))

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def transform_text(text):
  text = text.lower()
  text = nltk.word_tokenize(text)

  y = []
  for i in text:
    if i.isalnum():
      y.append(i)
  text = y[:]
  y.clear()

  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)

      text = y[:]
    y.clear()

    for i in text:
      y.append(ps.stem(i))
  return " ".join(y)

transform_text("I loved the YT lecture on Machine Learning. How about you?")

df['text'][2000]

df['transformed_text'] = df['text'].apply(transform_text)

df.head()

from wordcloud import wordcloud
WordCloud = wordcloud.WordCloud(width=500, height=500, min_font_size=10, background_color='white')

spam_wc = WordCloud.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=" "))

plt.imshow(spam_wc)

ham_wc = WordCloud.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=" "))

plt.imshow(ham_wc)
#

spam_corpus = []
for msg in df[df['target'] == 1]['transformed_text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)

len(spam_corpus)

from collections import Counter
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a DataFrame for the top 30 words
top_30 = pd.DataFrame(Counter(spam_corpus).most_common(30))
top_30.columns = ['Word', 'Frequency']

# Plot using seaborn
plt.figure(figsize=(10, 6))
sns.barplot(x='Word', y='Frequency', data=top_30, palette='viridis')
plt.xticks(rotation=90)
plt.title("Top 30 Most Common Words in Spam Messages")
plt.show()

ham_corpus = []
for msg in df[df['target'] == 0]['transformed_text'].tolist():
  for word in msg.split():
    ham_corpus.append(word)

len(ham_corpus)

from collections import Counter
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a DataFrame for the top 30 words
top_30 = pd.DataFrame(Counter(spam_corpus).most_common(30))
top_30.columns = ['Word', 'Frequency']

# Plot using seaborn
plt.figure(figsize=(10, 6))
sns.barplot(x='Word', y='Frequency', data=top_30, palette='viridis')
plt.xticks(rotation=90)
plt.title("Top 30 Most Common Words in Spam Messages")
plt.show()

"""**Model Bunilding**"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
cv = CountVectorizer()
tfidf = TfidfVectorizer(max_features=3000)

X = cv.fit_transform(df['transformed_text']).toarray()

X.shape

y = df['target'].values

y

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

gnb.fit(X_train, y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test, y_pred1))
print(confusion_matrix(y_test, y_pred1))
print(precision_score(y_test, y_pred1))

mnb.fit(X_train, y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test, y_pred2))
print(confusion_matrix(y_test, y_pred2))
print(precision_score(y_test, y_pred2))

bnb.fit(X_train, y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test, y_pred3))
print(confusion_matrix(y_test, y_pred3))
print(precision_score(y_test, y_pred3))

import joblib
joblib.dump(bnb, 'spam_classifier_model.joblib')
print("Model saved successfully")